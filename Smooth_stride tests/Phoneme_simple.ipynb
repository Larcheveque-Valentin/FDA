{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import Hyper_parameter_search\n",
    "from Hyper_parameter_search import Smoothing_method,weights_init_normal,HyperParameters,Hyperparameter_Search,Hyperparameter_Test,Hyper_parameter_GridSearch,FCNN,Compile_class,Compile_train\n",
    "\n",
    "import inspect\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "# matplotlib.use('TkAgg')\n",
    "from matplotlib import pyplot as plt\n",
    "import skfda as fda\n",
    "from skfda import representation as representation\n",
    "from skfda.exploratory.visualization import FPCAPlot\n",
    "from skfda.exploratory.visualization import FPCAPlot\n",
    "from skfda.preprocessing.dim_reduction import FPCA\n",
    "from skfda.representation.basis import BSpline, Fourier, Monomial\n",
    "import scipy\n",
    "from scipy.interpolate import BSpline\n",
    "import os\n",
    "import ignite\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "import random\n",
    "from random import seed\n",
    "from scipy import stats\n",
    "import statistics\n",
    "from statistics import stdev\n",
    "import gc\n",
    "import skfda\n",
    "from skfda import FDataGrid as fd\n",
    "from skfda.representation.basis import BSpline as B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilisateur\\anaconda3\\lib\\site-packages\\rdata\\conversion\\_conversion.py:321: UserWarning: Unknown encoding. Assumed ASCII.\n",
      "  warnings.warn(\"Unknown encoding. Assumed ASCII.\")\n",
      "c:\\Users\\Utilisateur\\anaconda3\\lib\\site-packages\\rdata\\conversion\\_conversion.py:321: UserWarning: Unknown encoding. Assumed ASCII.\n",
      "  warnings.warn(\"Unknown encoding. Assumed ASCII.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((250,),\n",
       " (250,),\n",
       " Index(['1', '2', '3', '4', '5'], dtype='object'),\n",
       " Index(['1', '2', '3', '4', '5'], dtype='object'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Phoneme=skfda.datasets.fetch_cran(\"phoneme\",package_name=\"fda.usc\")\n",
    "phoneme=Phoneme['phoneme']\n",
    "\n",
    "x_train,x_test,y_train,y_test=phoneme['learn'],phoneme['test'],phoneme['classlearn'],phoneme['classtest']\n",
    "x_train.shape,x_test.shape,y_train.categories,y_test.categories\n",
    "##Phonèmes enregistrés à classer dans 5 catégories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train_tensor=torch.tensor(x_train.data_matrix).reshape(250,1,150).cuda().float()\n",
    "x_test_tensor=torch.tensor(x_test.data_matrix).reshape(250,1,150).cuda().float()\n",
    "y_train_tensor=torch.tensor(y_train.codes).unsqueeze(1).unsqueeze(2).long().cuda()\n",
    "y_test_tensor=torch.tensor(y_test.codes).unsqueeze(1).unsqueeze(2).long().cuda()\n",
    "\n",
    "X=torch.cat([x_train_tensor,x_test_tensor])\n",
    "Y=torch.cat([y_train_tensor,y_test_tensor])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choix de la méthode de smoothing on créer une instance de la classe Smoothing_method(), ici par exemple on choisit la base de Bspline et un smoothing au lieu de l'interpolation. On choisit la base avec les noeufs et l'ordre de spline, à noter que l'on peut également le choisir avec le nombre de fonctions de basis en mettant les knots_or_basis=\"basis\" et n_basis=..., par défaut knots_or_basis=\"knots\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoother=Smoothing_method(Mode='Smoothing',basis_type=\"Bspline\",knots=np.linspace(1,12,8),B_spline_order=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création des hyperparamètres: Le NN a trois couches et les paramètres de chaque couche sont modifiables dans l'instance de la classe HyperParameters(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params=HyperParameters(negative_slope=0.2,Smoothing_method=smoother \n",
    ")\n",
    "params.basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoother=Smoothing_method(Mode='Smoothing',basis_type=\"Bspline\",knots=np.linspace(1,12,8),B_spline_order=4)\n",
    "\n",
    "params=HyperParameters(negative_slope=0.2,Smoothing_method=smoother \n",
    ")\n",
    "params.basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TSCNN(nn.Module):\n",
    "    def __init__(self, hyperparams,output_size):\n",
    "        super(TSCNN, self).__init__()\n",
    "        n_conv_out =hyperparams.n_conv_out\n",
    "        basis = hyperparams.basis\n",
    "        granulation = hyperparams.granulation\n",
    "        n_conv_in = hyperparams.n_conv_in\n",
    "        n_conv_in2 = hyperparams.n_conv_in2\n",
    "        n_conv_in3 = hyperparams.n_conv_in3\n",
    "        n_Flat_out = hyperparams.n_Flat_out\n",
    "        stride_1 = hyperparams.stride_1\n",
    "        stride_2 = hyperparams.stride_2\n",
    "        stride_3 = hyperparams.stride_3\n",
    "        stride_pool_1 = hyperparams.stride_pool_1\n",
    "        stride_pool_2 = hyperparams.stride_pool_2\n",
    "        stride_pool_3 = hyperparams.stride_pool_3\n",
    "        kernel_size_1 = hyperparams.kernel_size_1\n",
    "        kernel_size_2 = hyperparams.kernel_size_2\n",
    "        kernel_size_3 = hyperparams.kernel_size_3\n",
    "        kernel_size_pool_1 = hyperparams.kernel_size_pool_1\n",
    "        kernel_size_pool_2 = hyperparams.kernel_size_pool_2\n",
    "        kernel_size_pool_3 = hyperparams.kernel_size_pool_3\n",
    "        dilation_1 = hyperparams.dilation_1\n",
    "        dilation_2 = hyperparams.dilation_2\n",
    "        dilation_3 = hyperparams.dilation_3\n",
    "        dilation_pool_1 = hyperparams.dilation_pool_1\n",
    "        dilation_pool_2 = hyperparams.dilation_pool_2\n",
    "        dilation_pool_3 = hyperparams.dilation_pool_3\n",
    "        basis=hyperparams.basis\n",
    "        negative_slope=hyperparams.negative_slope\n",
    "        padding_1 = hyperparams.padding_1\n",
    "        padding_2 = hyperparams.padding_2\n",
    "        padding_3 = hyperparams.padding_3\n",
    "        padding_pool_1 = hyperparams.padding_pool_1\n",
    "        padding_pool_2 = hyperparams.padding_pool_2\n",
    "        padding_pool_3 = hyperparams.padding_pool_3\n",
    "        negative_slope = hyperparams.negative_slope\n",
    "        \n",
    "        # Reste du code pour l'initialisation de la classe model\n",
    "        self.basis=basis\n",
    "        self.granulation=granulation\n",
    "        self.convlayer1=nn.Sequential(\n",
    "            nn.Conv1d(1,n_conv_in,kernel_size=kernel_size_1,stride=stride_1,padding=padding_1,dilation=dilation_1),\n",
    "            nn.BatchNorm1d(n_conv_in),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "            hyperparams.activation,\n",
    "            nn.MaxPool1d(kernel_size=kernel_size_pool_1,stride=stride_pool_1,padding=padding_pool_1,dilation=dilation_pool_1),\n",
    "            nn.BatchNorm1d(n_conv_in),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "        )\n",
    "        \n",
    "        self.convlayer2=nn.Sequential(\n",
    "            nn.Conv1d(n_conv_in,n_conv_in2,kernel_size=kernel_size_2,stride=stride_2,padding=padding_2,dilation=dilation_2),\n",
    "            nn.BatchNorm1d(n_conv_in2),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "            hyperparams.activation,\n",
    "            nn.MaxPool1d(kernel_size=kernel_size_pool_2,stride=stride_pool_2,padding=padding_pool_2,dilation=dilation_pool_2),\n",
    "            nn.BatchNorm1d(n_conv_in2),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "        )\n",
    "        \n",
    "        self.convlayer3=nn.Sequential(\n",
    "\n",
    "            nn.Conv1d(n_conv_in2,n_conv_in3,kernel_size=kernel_size_3,stride=stride_3,padding=padding_3,dilation=dilation_3),\n",
    "            nn.BatchNorm1d(n_conv_in3),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "            hyperparams.activation,\n",
    "            nn.MaxPool1d(kernel_size=kernel_size_pool_3,stride=stride_pool_3,padding=padding_pool_3,dilation=dilation_pool_3),\n",
    "            nn.BatchNorm1d(n_conv_in3),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "        )\n",
    "\n",
    "        self.fc_block=nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(n_conv_out*n_conv_in3,n_Flat_out),\n",
    "            nn.BatchNorm1d(n_Flat_out),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "            hyperparams.activation,\n",
    "            nn.Linear(n_Flat_out,output_size),\n",
    "            \n",
    "        )\n",
    "        self.n_conv_out=n_conv_out\n",
    "        self.Smoothing_type=hyperparams.Smoothing_type\n",
    "        self.hyperparameters=hyperparams\n",
    "        self.smoother=hyperparams.Smoothing_method\n",
    "        \n",
    "    def Granulator(self,x):\n",
    "        ##On choisit ici le mode de smoothing, le premier cas c'est le smoothing\n",
    "        if \"inter\" not in self.Smoothing_type:     \n",
    "        ## Puis on sépare les deux cas selon si l'objet donné est Fdatagrid ou torch.tensor\n",
    "            if isinstance(x,skfda.representation.grid.FDataGrid):\n",
    "                eval_points=linspace(1,x.grid_points[0].shape[0],self.granulation)\n",
    "                coefs=x.to_basis(basis=self.basis).coefficients\n",
    "                basis_eval=self.basis.evaluate(eval_points=eval_points)[:, :, 0]\n",
    "                basis_fc = torch.from_numpy(basis_eval).float().cuda()\n",
    "                \n",
    "            elif isinstance(x,torch.Tensor):\n",
    "                eval_points=linspace(1,x.shape[2],self.granulation)\n",
    "                basis_eval=self.basis.evaluate(eval_points=eval_points)[:, :, 0]\n",
    "                basis_fc = torch.from_numpy(basis_eval).float().cuda()\n",
    "                coefs=fd(x[:,0,:].cpu(),grid_points=np.arange(x.shape[2]+1)[1:]).to_basis(basis=self.basis).coefficients\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"the NN argument must be either torch.tensor or skfda.representation.grid.FDataGrid\")\n",
    "        \n",
    "                \n",
    "            coefs_torch=torch.tensor(coefs).float().cuda()\n",
    "            Recons_train=torch.matmul(coefs_torch,basis_fc)\n",
    "            Recons_train=Recons_train.reshape(Recons_train.shape[0],1,Recons_train.shape[1])\n",
    "        ##On choisit ici le mode de smoothing, le deuxième cas c'est l'interpolation \n",
    "        ## Ici l'hypothèse faite est que les observations ne comportent pas d'erreur.\n",
    "\n",
    "        else:## Puis on sépare les deux cas selon si l'objet donné est Fdatagrid ou torch.tensor\n",
    "            if isinstance(x,skfda.representation.grid.FDataGrid):\n",
    "                x.interpolation=self.smoother.smoothing()\n",
    "                eval_points=linspace(1,x.grid_points[0].shape[0],self.granulation)\n",
    "                Recons_train=x.interpolation._evaluate(fdata=x,eval_points=eval_points)[:,:,0]\n",
    "                Recons_train=torch.tensor(Recons_train).reshape(Recons_train.shape[0],1,Recons_train.shape[1])\n",
    "            if isinstance(x,torch.Tensor):\n",
    "                grid=fd(x[:,0,:].cpu(),grid_points=np.arange(x.shape[2]+1)[1:])\n",
    "                eval_points=linspace(1,x.shape[2],self.granulation)\n",
    "                grid.interpolation=self.smoother.smoothing()\n",
    "                Recons_train=grid.interpolation._evaluate(fdata=grid,eval_points=eval_points)\n",
    "                Recons_train=torch.tensor(Recons_train).reshape(Recons_train.shape[0],1,Recons_train.shape[1])\n",
    "            else:\n",
    "                raise ValueError(\"the NN argument must be either torch.tensor or skfda.representation.grid.FDataGrid\")\n",
    "        \n",
    "\n",
    "        return Recons_train.float().cuda()\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        Granulated_x_train=self.Granulator(x)\n",
    "        tresh_out=torch.relu(Granulated_x_train)\n",
    "        Conv_out=self.convlayer1(tresh_out)\n",
    "        Conv_out2=self.convlayer2(Conv_out)\n",
    "        Conv_out3=self.convlayer3(Conv_out2)\n",
    "        Lin_out=self.fc_block(Conv_out3)\n",
    "        return Lin_out.float().unsqueeze_(2).unsqueeze_(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'une instance de la classe: creer la classe demande deux choses: Les hyperparamètres (objet de la class HyperParameters) et la taille de la couche finale (ici pour les données phonèmes output_size=5 car il y a 5 catégories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utilisateur\\AppData\\Local\\Temp\\ipykernel_21068\\1589760509.py:92: DeprecationWarning: The method 'evaluate' is deprecated. Please use the normal calling notation on the basis object instead.\n",
      "  basis_eval=self.basis.evaluate(eval_points=eval_points)[:, :, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "TSC_model=TSCNN(hyperparams=params,output_size=5).cuda().apply(weights_init_normal)\n",
    "Granul=TSC_model.Granulator(x_train)\n",
    "Conv_out=TSC_model.convlayer1(Granul)\n",
    "Conv_out2=TSC_model.convlayer2(Conv_out)\n",
    "Conv_out3=TSC_model.convlayer3(Conv_out2)\n",
    "n_conv_out1=Conv_out.shape[2]\n",
    "n_conv_out2=Conv_out2.shape[2]\n",
    "n_conv_out3=Conv_out3.shape[2]\n",
    "n_conv_out=n_conv_out3\n",
    "print(n_conv_out3)\n",
    "params.n_conv_out=n_conv_out\n",
    "TSC_model=TSCNN(hyperparams=params,output_size=5).cuda().apply(weights_init_normal)\n",
    "# TSC_model.fc_block,TSC_model.n_conv_out*params.n_conv_in3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=params.opt\n",
    "lr=params.lr\n",
    "loss=params.loss\n",
    "batch_size=params.batch_size\n",
    "betas = [0.5, 0.999]\n",
    "if opt == \"Adam\":\n",
    "    optimizer = optim.Adam(TSC_model.parameters(), lr=lr, betas=betas)\n",
    "else:\n",
    "    optimizer = optim.SGD(TSC_model.parameters(), lr=lr)\n",
    "def train(n_epochs=params.n_epochs, module=TSC_model, optimizer=optimizer, loss=loss, batch_size=batch_size):\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        train_loss = torch.tensor(0).cuda().long()\n",
    "        \n",
    "        # Mélanger les données d'entraînement\n",
    "        indices = list(range(len(x_train)))\n",
    "        random.shuffle(indices)\n",
    "        \n",
    "        batch_index = 0  # Indice de batch\n",
    "        \n",
    "        for i in range(int(len(x_train) / batch_size)):\n",
    "            # Obtenir les indices des données mélangées\n",
    "            batch_indices = indices[batch_index:batch_index+batch_size]\n",
    "            \n",
    "            functions_train = x_train[batch_indices]\n",
    "            labels_train = y_train_tensor[batch_indices]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = module(functions_train)\n",
    "            loss_value = loss(input=output, target=labels_train)\n",
    "            \n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss_value.long()\n",
    "            \n",
    "            batch_index += batch_size  # Passer au prochain batch\n",
    "            \n",
    "        return train_loss, loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]C:\\Users\\Utilisateur\\AppData\\Local\\Temp\\ipykernel_21068\\1589760509.py:92: DeprecationWarning: The method 'evaluate' is deprecated. Please use the normal calling notation on the basis object instead.\n",
      "  basis_eval=self.basis.evaluate(eval_points=eval_points)[:, :, 0]\n",
      "  0%|          | 0/1 [00:03<?, ?it/s]\n",
      "C:\\Users\\Utilisateur\\AppData\\Local\\Temp\\ipykernel_21068\\1589760509.py:97: DeprecationWarning: The method 'evaluate' is deprecated. Please use the normal calling notation on the basis object instead.\n",
      "  basis_eval=self.basis.evaluate(eval_points=eval_points)[:, :, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision = 93.600006 %\n"
     ]
    }
   ],
   "source": [
    "train(n_epochs=1,module=TSC_model)\n",
    "accuracy=((torch.sum(torch.argmax(TSC_model(x_test_tensor),dim=1)==y_test_tensor)/x_test_tensor.shape[0])*100)\n",
    "print(\"Précision =\",accuracy.detach().cpu().numpy(),\"%\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utilisateur\\AppData\\Local\\Temp\\ipykernel_21068\\1589760509.py:97: DeprecationWarning: The method 'evaluate' is deprecated. Please use the normal calling notation on the basis object instead.\n",
      "  basis_eval=self.basis.evaluate(eval_points=eval_points)[:, :, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision = 93.600006 %\n"
     ]
    }
   ],
   "source": [
    "accuracy=((torch.sum(torch.argmax(TSC_model(x_test_tensor),dim=1)==y_test_tensor)/x_test_tensor.shape[0])*100)\n",
    "print(\"Précision =\",accuracy.detach().cpu().numpy(),\"%\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
