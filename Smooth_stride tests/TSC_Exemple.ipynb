{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import Functional_Data_functions\n",
    "from Functional_Data_functions import Smoothing_method,weights_init_normal,HyperParameters,Hyperparameter_Search,Hyperparameter_Test,Hyper_parameter_GridSearch,FCNN,Compile_class,Compile_train,from_torch_to_Datagrid\n",
    "\n",
    "import inspect\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "# matplotlib.use('TkAgg')\n",
    "from matplotlib import pyplot as plt\n",
    "import skfda as fda\n",
    "from skfda import representation as representation\n",
    "from skfda.exploratory.visualization import FPCAPlot\n",
    "from skfda.exploratory.visualization import FPCAPlot\n",
    "from skfda.preprocessing.dim_reduction import FPCA\n",
    "from skfda.representation.basis import BSpline, Fourier, Monomial\n",
    "import scipy\n",
    "from scipy.interpolate import BSpline\n",
    "import os\n",
    "import ignite\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "import random\n",
    "from random import seed\n",
    "from scipy import stats\n",
    "import statistics\n",
    "from statistics import stdev\n",
    "import gc\n",
    "import skfda\n",
    "from skfda import FDataGrid as fd\n",
    "from skfda.representation.basis import BSpline as B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilisateur\\anaconda3\\lib\\site-packages\\rdata\\conversion\\_conversion.py:321: UserWarning: Unknown encoding. Assumed ASCII.\n",
      "  warnings.warn(\"Unknown encoding. Assumed ASCII.\")\n",
      "c:\\Users\\Utilisateur\\anaconda3\\lib\\site-packages\\rdata\\conversion\\_conversion.py:321: UserWarning: Unknown encoding. Assumed ASCII.\n",
      "  warnings.warn(\"Unknown encoding. Assumed ASCII.\")\n"
     ]
    }
   ],
   "source": [
    "Phoneme=skfda.datasets.fetch_cran(\"phoneme\",package_name=\"fda.usc\")\n",
    "phoneme=Phoneme['phoneme']\n",
    "randices=np.random.randint(150,size=150)\n",
    "# print(randices)\n",
    "x_train,x_test,y_train,y_test=phoneme['learn'],phoneme['test'],phoneme['classlearn'],phoneme['classtest']\n",
    "x_train.shape,x_test.shape,y_train.categories,y_test.categories\n",
    "##Phonèmes enregistrés à classer dans 5 catégories\n",
    "x_train,x_test,y_train,y_test=x_train[randices],x_test[randices],y_train[randices],y_test[randices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor=torch.tensor(x_train.data_matrix).reshape(x_train.shape[0],1,150) .float()\n",
    "\n",
    "x_test_tensor=torch.tensor(x_test.data_matrix).reshape(x_test.shape[0],1,150) .float()\n",
    "y_train_tensor=torch.tensor(y_train.codes).unsqueeze(1).unsqueeze(2).long() \n",
    "y_test_tensor=torch.tensor(y_test.codes).unsqueeze(1).unsqueeze(2).long() \n",
    "X=torch.cat([x_train_tensor,x_test_tensor])\n",
    "Y=torch.cat([y_train_tensor,y_test_tensor])\n",
    "XY=torch.cat([X,Y],dim=2)\n",
    "def from_torch_to_Datagrid(x):\n",
    "    if isinstance(x,torch.Tensor):\n",
    "        x_grid=fd(x[:,0,:].cpu(),grid_points=np.arange(x.shape[2]+1)[1:])\n",
    "    elif isinstance(x,skfda.representation.grid.FDataGrid):\n",
    "        x_grid=x\n",
    "    else:\n",
    "        raise ValueError(\"the NN argument must be either torch.tensor or skfda.representation.grid.FDataGrid\")\n",
    "    \n",
    "    return x_grid\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choix de la méthode de smoothing on créer une instance de la classe Smoothing_method(), ici par exemple on choisit la base de Bspline et un smoothing au lieu de l'interpolation. On choisit la base avec les noeufs et l'ordre de spline, à noter que l'on peut également le choisir avec le nombre de fonctions de basis en mettant les knots_or_basis=\"basis\" et n_basis=..., par défaut knots_or_basis=\"knots\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis=B(knots=linspace(1,150,6),order=3)\n",
    "##taille de la grille d'évaluation :\n",
    "n=7500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création des hyperparamètres: Le NN a trois couches et les paramètres de chaque couche sont modifiables dans l'instance de la classe HyperParameters(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_slope=0.17\n",
    "class TSCNN(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(TSCNN, self).__init__()\n",
    "    \n",
    "        # Reste du code pour l'initialisation de la classe model\n",
    "        self.convlayer1=nn.Sequential(\n",
    "            nn.Conv1d(1,8,kernel_size=7,stride=95,padding=200,dilation=75),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "\n",
    "            nn.Dropout(0.2),\n",
    "            nn.MaxPool1d(kernel_size=3,stride=1,padding=0,dilation=1),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        \n",
    "        self.convlayer2=nn.Sequential(\n",
    "            nn.Conv1d(8,512,kernel_size=5,stride=3,padding=2,dilation=2),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.MaxPool1d(kernel_size=3,stride=2,padding=1,dilation=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        \n",
    "        self.convlayer3=nn.Sequential(\n",
    "\n",
    "            nn.Conv1d(512,128,kernel_size=2,stride=1,padding=2,dilation=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "            \n",
    "            nn.MaxPool1d(kernel_size=2,stride=1,padding=0,dilation=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "        )\n",
    "\n",
    "        self.fc_block=nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(15*128,256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "            nn.Linear(256,5),\n",
    "            \n",
    "        )\n",
    "        \n",
    "    def Granulator(self,x):\n",
    "        x_grid=from_torch_to_Datagrid(x=x)\n",
    "        \n",
    "        Recons_train=torch.zeros([x_grid.shape[0],1,n]).float() \n",
    "        eval_points=linspace(1,x_grid.grid_points[0].shape[0],n)\n",
    "        coefs_torch=torch.from_numpy(x_grid.to_basis(basis=basis).coefficients).float() \n",
    "        basis_eval=basis(eval_points=eval_points,derivative=0)\n",
    "        basis_fc = torch.from_numpy(basis_eval).float()         \n",
    "        Recons_train=torch.matmul(coefs_torch,basis_fc[:,:,0])\n",
    "        Recons_train=Recons_train.reshape(Recons_train.shape[0],1,n)\n",
    "           \n",
    "    \n",
    "        return Recons_train.float() \n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        Granulated_x_train=self.Granulator(x)\n",
    "        Conv_out=self.convlayer1(Granulated_x_train)\n",
    "        Conv_out2=self.convlayer2(Conv_out)\n",
    "        Conv_out3=self.convlayer3(Conv_out2)\n",
    "        Lin_out=self.fc_block(Conv_out3)\n",
    "        return Lin_out.float().unsqueeze_(2).unsqueeze_(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taille de la sortie de convolution finale= 15\n",
      "Shape de la sortie du modèle= torch.Size([300, 5, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "#Création d'une instance de la classe:\n",
    "TSC_model=TSCNN().apply(weights_init_normal)\n",
    "Granul=TSC_model.Granulator(x_train)\n",
    "Conv_out=TSC_model.convlayer1(Granul)\n",
    "Conv_out2=TSC_model.convlayer2(Conv_out)\n",
    "Conv_out3=TSC_model.convlayer3(Conv_out2)\n",
    "n_conv_out1=Conv_out.shape[2]\n",
    "n_conv_out2=Conv_out2.shape[2]\n",
    "n_conv_out3=Conv_out3.shape[2]\n",
    "##ON prend compte de la taille de la sortie \n",
    "# de la convolution pour actuliser la taille de la couche linéaire.\n",
    "n_conv_out=n_conv_out3\n",
    "print(\"taille de la sortie de convolution finale=\",n_conv_out3)\n",
    "print(\"Shape de la sortie du modèle=\",TSC_model(X).shape)\n",
    "# TSC_model.fc_block,TSC_model.n_conv_out*params.n_conv_in3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=\"SGD\"\n",
    "lr=0.00089\n",
    "loss=nn.CrossEntropyLoss()\n",
    "batch_size=100\n",
    "betas = [0.5, 0.999]\n",
    "if opt == \"Adam\":\n",
    "    optimizer = optim.Adam(TSC_model.parameters(), lr=lr, betas=betas)\n",
    "else:\n",
    "    optimizer = optim.SGD(TSC_model.parameters(), lr=lr)\n",
    "def train(n_epochs=1, module=TSC_model, optimizer=optimizer, loss=loss, batch_size=batch_size):\n",
    "    training_acc=torch.tensor([0])\n",
    "    testing_acc=torch.tensor([0])\n",
    "    \n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        train_loss = torch.tensor(0) .long()\n",
    "        \n",
    "        # Mélanger les données d'entraînement\n",
    "        indices = list(range(len(x_train)))\n",
    "        random.shuffle(indices)\n",
    "        \n",
    "        batch_index = 0  # Indice de batch\n",
    "        \n",
    "        for i in range(int(len(x_train) / batch_size)):\n",
    "            # Obtenir les indices des données mélangées\n",
    "            batch_indices = indices[batch_index:batch_index+batch_size]\n",
    "            \n",
    "            functions_train = x_train[batch_indices]\n",
    "            labels_train = y_train_tensor[batch_indices]\n",
    "            optimizer.zero_grad()\n",
    "            output = module(functions_train)\n",
    "            loss_value = loss(input=output, target=labels_train)\n",
    "            \n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss_value.long()\n",
    "            batch_index += batch_size  # Passer au prochain batch\n",
    "\n",
    "        accuracy_training=((torch.sum(torch.argmax(TSC_model(x_train),dim=1)==y_train_tensor)/x_train_tensor.shape[0])*100).cpu().unsqueeze(0)\n",
    "        accuracy=((torch.sum(torch.argmax(TSC_model(x_test_tensor),dim=1)==y_test_tensor)/x_test_tensor.shape[0])*100).cpu().unsqueeze(0)\n",
    "        testing_acc=torch.cat([testing_acc,accuracy],dim=0)\n",
    "        training_acc=torch.cat([training_acc,accuracy_training],dim=0)\n",
    "        \n",
    "    return training_acc,testing_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  6.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision = 99.333336 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "training_acc,testing_acc=train(n_epochs=20,module=TSC_model)\n",
    "accuracy=((torch.sum(torch.argmax(TSC_model(x_test_tensor),dim=1)==y_test_tensor)/x_test_tensor.shape[0])*100)\n",
    "print(\"Précision =\",accuracy.detach().cpu().numpy(),\"%\")  \n",
    "# accuracy.unsqueeze(0)\n",
    "\n",
    "# training_acc=torch.cat([accuracy.cpu().unsqueeze(0),training_acc],dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4478"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
